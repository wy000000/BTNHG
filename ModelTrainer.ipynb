{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c2b43bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start import\n",
      "import used time:  17.39837884902954\n",
      "当前时间: 11-25 12:36:39\n"
     ]
    }
   ],
   "source": [
    "print(\"start import\")\n",
    "import time\n",
    "time1 = time.time()\n",
    "import torch\n",
    "from torch_geometric.data import Data # 从torch_geometric.data导入Data类，用于表示图数据\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "time2 = time.time()\n",
    "print(\"import used time: \", time2 - time1)\n",
    "print(f\"当前时间: {time.strftime('%m-%d %H:%M:%S', time.localtime())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fbd5954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start construct HeteroData\n",
      "start read data\n",
      "读取数据用时: 3.0216689109802246\n",
      "当前时间: 11-25 12:36:50\n",
      "construct ID map and 构建节点特征矩阵\n",
      "构建ID map and 节点特征矩阵用时: 1.2473902702331543\n",
      "当前时间: 11-25 12:36:51\n",
      "构建边关系\n",
      "构建边关系用时: 11.475317239761353\n",
      "当前时间: 11-25 12:37:03\n",
      "给 address 节点加标签\n",
      "给 address 节点加标签用时:  0.3993222713470459\n",
      "转成无向图\n",
      "当前边数: 5971294\n",
      "make undirected graph用时: 0.042025089263916016\n",
      "当前时间: 11-25 12:37:03\n",
      "当前边数: 11942588\n",
      "转无向图用时: 0.04263186454772949\n",
      "当前时间: 11-25 12:37:03\n"
     ]
    }
   ],
   "source": [
    "from BTNHGV2ParameterClass import BTNHGV2ParameterClass\n",
    "from BTNHGV2HeteroDataClass import BTNHGV2HeteroDataClass\n",
    "heteroDataClass=BTNHGV2HeteroDataClass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ce67e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "划分数据集信息\n",
      "训练集大小: 12174 (80.00%)\n",
      "测试集大小: 3044 (20.00%)\n",
      "划分数据集用时: 0.03703784942626953\n",
      "当前时间: 11-25 12:37:07\n",
      "using device: cuda\n",
      "start train\n",
      "x_dict[address]: torch.Size([546649, 13])\n",
      "x_dict after conv1: torch.Size([546649, 32])\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 830.00 MiB. GPU 0 has a total capacity of 6.00 GiB of which 0 bytes is free. Of the allocated memory 11.72 GiB is allocated by PyTorch, and 533.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     15\u001b[39m gmodel=HAN(heteroDataCls=heteroDataClass,\n\u001b[32m     16\u001b[39m \t\t\thidden_channels=BTNHGV2ParameterClass.hidden_channels,\n\u001b[32m     17\u001b[39m \t\t\tout_channels=BTNHGV2ParameterClass.out_channels,\n\u001b[32m     18\u001b[39m \t\t\tnum_heads=BTNHGV2ParameterClass.num_heads,\n\u001b[32m     19\u001b[39m \t\t\tdropout=BTNHGV2ParameterClass.dropout\n\u001b[32m     20\u001b[39m \t\t\t)\n\u001b[32m     22\u001b[39m trainer=ModelTrainerClass(model=gmodel,\n\u001b[32m     23\u001b[39m \t\t\t\t\t\t  \tdevice=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     24\u001b[39m \t\t\t\t\t\t\tlr=BTNHGV2ParameterClass.lr,\n\u001b[32m   (...)\u001b[39m\u001b[32m     28\u001b[39m \t\t\t\t\t\t\tloss_threshold=BTNHGV2ParameterClass.loss_threshold\n\u001b[32m     29\u001b[39m \t\t\t\t\t\t)\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m trainer.test()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\wy\\source\\repos\\BTNHG\\BTNHG\\ModelTrainerClass.py:126\u001b[39m, in \u001b[36mModelTrainerClass.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    124\u001b[39m epoch=\u001b[32m0\u001b[39m\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, \u001b[38;5;28mself\u001b[39m._epochs + \u001b[32m1\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m \tloss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_train_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    127\u001b[39m \t\u001b[38;5;28;01mif\u001b[39;00m(epoch%\u001b[32m10\u001b[39m==\u001b[32m0\u001b[39m):\n\u001b[32m    128\u001b[39m \t\t\u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m03d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\wy\\source\\repos\\BTNHG\\BTNHG\\ModelTrainerClass.py:81\u001b[39m, in \u001b[36mModelTrainerClass._train_one_epoch\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     78\u001b[39m chunk_mask[chunk_indices] = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     80\u001b[39m \u001b[38;5;66;03m# 前向传播和损失计算\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m outAddress = out[\u001b[33m'\u001b[39m\u001b[33maddress\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     83\u001b[39m loss = F.cross_entropy(outAddress[chunk_mask], \n\u001b[32m     84\u001b[39m \t\t\t\t\t\u001b[38;5;28mself\u001b[39m._model.heteroData[\u001b[33m\"\u001b[39m\u001b[33maddress\u001b[39m\u001b[33m\"\u001b[39m].y[chunk_mask])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\wy\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\wy\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\wy\\source\\repos\\BTNHG\\BTNHG\\HAN.py:71\u001b[39m, in \u001b[36mHAN.forward\u001b[39m\u001b[34m(self, x_dict, edge_index_dict)\u001b[39m\n\u001b[32m     69\u001b[39m x_dict = {k: F.dropout(v, p=\u001b[38;5;28mself\u001b[39m._dropout, training=\u001b[38;5;28mself\u001b[39m.training) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m x_dict.items()}\n\u001b[32m     70\u001b[39m \u001b[38;5;66;03m# 第二次卷积\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m x_dict = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mx_dict[address] after conv2: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx_dict[\u001b[33m\"\u001b[39m\u001b[33maddress\u001b[39m\u001b[33m\"\u001b[39m].shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     73\u001b[39m x_dict = {k: \u001b[38;5;28mself\u001b[39m.ln2(v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m x_dict.items()}\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\wy\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\wy\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\wy\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\torch_geometric\\nn\\conv\\han_conv.py:161\u001b[39m, in \u001b[36mHANConv.forward\u001b[39m\u001b[34m(self, x_dict, edge_index_dict, return_semantic_attention_weights)\u001b[39m\n\u001b[32m    159\u001b[39m semantic_attn_dict = {}\n\u001b[32m    160\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m node_type, outs \u001b[38;5;129;01min\u001b[39;00m out_dict.items():\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m     out, attn = \u001b[43mgroup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mouts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mk_lin\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    162\u001b[39m     out_dict[node_type] = out\n\u001b[32m    163\u001b[39m     semantic_attn_dict[node_type] = attn\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\wy\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\torch_geometric\\nn\\conv\\han_conv.py:28\u001b[39m, in \u001b[36mgroup\u001b[39m\u001b[34m(xs, q, k_lin)\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m out.numel() == \u001b[32m0\u001b[39m:\n\u001b[32m     27\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m out.view(\u001b[32m0\u001b[39m, out.size(-\u001b[32m1\u001b[39m)), \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m attn_score = (q * torch.tanh(\u001b[43mk_lin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m).mean(\u001b[32m1\u001b[39m)).sum(-\u001b[32m1\u001b[39m)\n\u001b[32m     29\u001b[39m attn = F.softmax(attn_score, dim=\u001b[32m0\u001b[39m)\n\u001b[32m     30\u001b[39m out = torch.sum(attn.view(num_edge_types, \u001b[32m1\u001b[39m, -\u001b[32m1\u001b[39m) * out, dim=\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\wy\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\wy\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\wy\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m    131\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03m    Runs the forward pass.\u001b[39;00m\n\u001b[32m    133\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 830.00 MiB. GPU 0 has a total capacity of 6.00 GiB of which 0 bytes is free. Of the allocated memory 11.72 GiB is allocated by PyTorch, and 533.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "from ModelTrainerClass import ModelTrainerClass\n",
    "from BTNHGV2ParameterClass import BTNHGV2ParameterClass\n",
    "from BTNHGV2HeteroDataClass import BTNHGV2HeteroDataClass\n",
    "from HAN import HAN\n",
    "import sys\n",
    "import importlib\n",
    "importlib.reload(sys.modules['ModelTrainerClass'])\n",
    "importlib.reload(sys.modules['BTNHGV2ParameterClass'])\n",
    "importlib.reload(sys.modules['HAN'])\n",
    "importlib.reload(sys.modules['BTNHGV2HeteroDataClass'])\n",
    "from BTNHGV2ParameterClass import BTNHGV2ParameterClass\n",
    "from BTNHGV2HeteroDataClass import BTNHGV2HeteroDataClass\n",
    "from ModelTrainerClass import ModelTrainerClass\n",
    "from HAN import HAN\n",
    "gmodel=HAN(heteroDataCls=heteroDataClass,\n",
    "\t\t\thidden_channels=BTNHGV2ParameterClass.hidden_channels,\n",
    "\t\t\tout_channels=BTNHGV2ParameterClass.out_channels,\n",
    "\t\t\tnum_heads=BTNHGV2ParameterClass.num_heads,\n",
    "\t\t\tdropout=BTNHGV2ParameterClass.dropout\n",
    "\t\t\t)\n",
    "\n",
    "trainer=ModelTrainerClass(model=gmodel,\n",
    "\t\t\t\t\t\t  \tdevice=None,\n",
    "\t\t\t\t\t\t\tlr=BTNHGV2ParameterClass.lr,\n",
    "\t\t\t\t\t\t\tweight_decay=BTNHGV2ParameterClass.weight_decay,\n",
    "\t\t\t\t\t\t\tepochs=BTNHGV2ParameterClass.epochs,\n",
    "\t\t\t\t\t\t\tpatience=BTNHGV2ParameterClass.patience,\n",
    "\t\t\t\t\t\t\tloss_threshold=BTNHGV2ParameterClass.loss_threshold\n",
    "\t\t\t\t\t\t)\n",
    "trainer.run()\n",
    "trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa6a411",
   "metadata": {},
   "outputs": [],
   "source": [
    "from BTNHGV2LoaderClass import BTNHGV2LoaderClass\n",
    "print(\"start split loader\")\n",
    "loaderClass=BTNHGV2LoaderClass(heteroData=heteroDataClass.heteroData)\n",
    "train_loader, test_loader = loaderClass.getTrainLoaderAndTestLoader(train_size=BTNHGV2ParameterClass.train_size,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tbatch_size=BTNHGV2ParameterClass.batch_size,#12174\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tshuffle=BTNHGV2ParameterClass.shuffle,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tisResetSeed=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a9519d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n",
      "Epoch 010 | Loss: 1.0361\n",
      "Epoch 020 | Loss: 0.7161\n",
      "Epoch 030 | Loss: 0.3467\n",
      "Epoch 040 | Loss: 0.0922\n",
      "Epoch 050 | Loss: 0.0213\n",
      "Epoch 060 | Loss: 0.0099\n",
      "Epoch 070 | Loss: 0.0061\n",
      "Epoch 080 | Loss: 0.0043\n",
      "Epoch 090 | Loss: 0.0030\n",
      "Epoch 100 | Loss: 0.0019\n",
      "训练完成, epoch : 100, loss: 0.0019\n",
      "Accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ModelTrainerClass import ModelTrainerClass\n",
    "from BTNHGV2ParameterClass import BTNHGV2ParameterClass\n",
    "from HAN import HAN\n",
    "import sys\n",
    "import importlib\n",
    "importlib.reload(sys.modules['ModelTrainerClass'])\n",
    "importlib.reload(sys.modules['BTNHGV2ParameterClass'])\n",
    "from BTNHGV2ParameterClass import BTNHGV2ParameterClass\n",
    "from ModelTrainerClass import ModelTrainerClass\n",
    "gmodel=HAN(\n",
    "\t\t\n",
    ")\n",
    "trainer=ModelTrainerClass(model=gmodel, train_loader=loader, test_loader=loader,\n",
    "\t\t\t\t\t\tlr=BTNHGV2ParameterClass.lr, patience=BTNHGV2ParameterClass.patience)\n",
    "trainer.run()\n",
    "trainer.test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cb2445",
   "metadata": {},
   "outputs": [],
   "source": [
    "from BTNHGV2ParameterClass import BTNHGV2ParameterClass\n",
    "from BTNHGV2HeteroDataClass import BTNHGV2HeteroDataClass\n",
    "from BTNHGV2LoaderClass import BTNHGV2LoaderClass\n",
    "from ModelTrainerClass import ModelTrainerClass\n",
    "import sys\n",
    "import importlib\n",
    "importlib.reload(sys.modules['BTNHGV2ParameterClass'])\n",
    "importlib.reload(sys.modules['BTNHGV2HeteroDataClass'])\n",
    "importlib.reload(sys.modules['BTNHGV2LoaderClass'])\n",
    "importlib.reload(sys.modules['ModelTrainerClass'])\n",
    "from BTNHGV2ParameterClass import BTNHGV2ParameterClass\n",
    "from BTNHGV2HeteroDataClass import BTNHGV2HeteroDataClass\n",
    "from BTNHGV2LoaderClass import BTNHGV2LoaderClass\n",
    "from ModelTrainerClass import ModelTrainerClass\n",
    "from HAN import HAN\n",
    "\n",
    "heteroDataCls = BTNHGV2HeteroDataClass()\n",
    "heteroData = heteroDataCls.heteroData\n",
    "loaderCls = BTNHGV2LoaderClass(heteroData=heteroData)\n",
    "train_loader, test_loader = loaderCls.getTrainLoaderAndTestLoader(\n",
    "\t\t\t\t\t\t\t\t\ttrain_size=BTNHGV2ParameterClass.train_ratio,\n",
    "\t\t\t\t\t\t\t\t\tbatch_size=BTNHGV2ParameterClass.batch_size,\n",
    "\t\t\t\t\t\t\t\t\tshuffle=BTNHGV2ParameterClass.shuffle,\n",
    "\t\t\t\t\t\t\t\t\tisResetSeed=False)\n",
    "hgmodel = HAN(\n",
    "\tmetadata=,\n",
    "\thidden_channels=,\n",
    "\tout_channels=,\n",
    "\tnum_heads=,\n",
    "\tdropout=,\n",
    "\tnum_classes=\n",
    ")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
